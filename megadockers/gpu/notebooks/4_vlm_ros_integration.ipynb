{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2313db-cc41-4589-b2d9-4974abe55ca7",
   "metadata": {},
   "source": [
    "# ROS Integration\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Run a local VLM as a service node using ROS2 Python RCL\n",
    "\n",
    "## References\n",
    "\n",
    "* [Understanding Services](https://docs.ros.org/en/kilted/Tutorials/Beginner-CLI-Tools/Understanding-ROS2-Services/Understanding-ROS2-Services.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5nfxw2y2m",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook demonstrates integrating a Vision Language Model (VLM) with ROS2 using llama.cpp as the inference backend.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. **llama.cpp server** running with a VLM model (e.g., LLaVA, BakLLaVA)\n",
    "2. **ROS2** environment sourced\n",
    "3. **vlm_ros package** built and sourced\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- **llama.cpp server**: Runs the VLM model and exposes HTTP API (port 8080)\n",
    "- **vlm_service node**: ROS2 node that subscribes to camera images and queries the VLM\n",
    "- **Test script**: Python script to publish test images and receive VLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oon1an65zn",
   "metadata": {},
   "source": [
    "## Start llama.cpp Server\n",
    "\n",
    "**In a separate terminal**, start the llama.cpp server with a VLM model:\n",
    "\n",
    "```bash\n",
    "# Example: Download and run a VLM model\n",
    "export PATH=/ryzers/llamacpp/build/bin/:$PATH\n",
    "llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8080\n",
    "```\n",
    "\n",
    "Wait until you see \"HTTP server listening\" before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c424a6e-d3fb-4231-b7bd-7e351a2ba3fc",
   "metadata": {},
   "source": [
    "## Try OpenAI API server directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b789064-a451-45e5-9b0e-7a89f44b5b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A black and white portrait of a woman wearing a wide brimmed hat with a feather sticking out of the right side.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "LLAMA_SERVER_URL = \"http://0.0.0.0:8080/v1/chat/completions\"\n",
    "\n",
    "def b64_image(image_path: str) -> str:\n",
    "    data = Path(image_path).read_bytes()\n",
    "    return base64.b64encode(data).decode(\"utf-8\")\n",
    "\n",
    "def ask_with_image(image_path: str, question: str):\n",
    "    payload = {\n",
    "        \"model\": \"smolvlm\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image(image_path)}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 256\n",
    "    }\n",
    "    \n",
    "    r = requests.post(LLAMA_SERVER_URL, json=payload, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    resp = r.json()\n",
    "    print(resp[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "ask_with_image(\"lena.jpg\", \"What's in this image? Be concise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmer6c51u1r",
   "metadata": {},
   "source": [
    "## Build the ROS Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48v7cqpall7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting >>> vlm_ros\n",
      "Finished <<< vlm_ros [0.59s]\n",
      "\n",
      "Summary: 1 package finished [0.67s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /opt/ros/kilted/setup.bash\n",
    "\n",
    "# Build the vlm_ros package\n",
    "cd /ryzers/notebooks/vlm_ros\n",
    "colcon build --symlink-install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x39wtvqpvmh",
   "metadata": {},
   "source": [
    "## Start the VLM Service Node\n",
    "\n",
    "Run the VLM service node in the background:\n",
    "\n",
    "```bash\n",
    "source /opt/ros/kilted/setup.bash\n",
    "source /ryzers/notebooks/vlm_ros/install/setup.bash\n",
    "ros2 run vlm_ros vlm_service\n",
    "```\n",
    "\n",
    "You should see this output:\n",
    "\n",
    "```\n",
    "[INFO] [1760575146.024196594] [vlm_service]: VLM Service started, connecting to http://localhost:8080\n",
    "```\n",
    "\n",
    "## Test with a Sample Image\n",
    "\n",
    "Create a test script to publish an image and receive VLM responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7qm76p1xo4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM tester node initialized\n"
     ]
    }
   ],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "from std_msgs.msg import String\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class VLMTester(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('vlm_tester')\n",
    "        self.image_pub = self.create_publisher(Image, 'camera/image', 10)\n",
    "        self.response_sub = self.create_subscription(String, 'vlm/response', self.response_callback, 10)\n",
    "        self.bridge = CvBridge()\n",
    "        self.latest_response = None\n",
    "        \n",
    "    def response_callback(self, msg):\n",
    "        self.latest_response = msg.data\n",
    "        print(f\"\\nVLM Response: {msg.data}\\n\")\n",
    "        \n",
    "    def publish_test_image(self, image_path):\n",
    "        # Load and publish image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Could not load image: {image_path}\")\n",
    "            return\n",
    "            \n",
    "        msg = self.bridge.cv2_to_imgmsg(img, encoding='bgr8')\n",
    "        self.image_pub.publish(msg)\n",
    "        print(f\"Published image: {image_path}\")\n",
    "\n",
    "# Initialize ROS\n",
    "rclpy.init()\n",
    "tester = VLMTester()\n",
    "print(\"VLM tester node initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neas4wx6t7",
   "metadata": {},
   "source": [
    "### Publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bvgsjmxc3j4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published image: /ryzers/notebooks/images/toucan.jpg\n",
      "\n",
      "VLM Response:  A bird with a blue patch on its head sits on a branch in a forest.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Publish a test image\n",
    "tester.publish_test_image('/ryzers/notebooks/images/toucan.jpg')\n",
    "rclpy.spin_once(tester, timeout_sec=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b05f71-7e8f-475a-836f-8d363490757b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Try launching llama-server with different models. See how a smaller model like `SmolVLM-256M-Instruct-GGUF` or a much larger SoTA model like `gemma-3-4b-it-GGUF`\n",
    "\n",
    "```\n",
    "llama-server -hf ggml-org/gemma-3-4b-it-GGUF\n",
    "llama-server -hf ggml-org/SmolVLM-256M-Instruct-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285bb33-051c-4edb-bb43-14e4efa00fde",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "CopyrightÂ© 2025 AMD, Inc SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
