{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5dcdaa-b22d-46d8-bed4-d89148a5cdc3",
   "metadata": {},
   "source": [
    "# Running Large Language Models\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Learn to use Llama.cpp to run inference on the AMD GPU\n",
    "* Learn to use Ollama to run inference on AMD GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4aafe-e673-44a6-82b6-82ac23e85763",
   "metadata": {},
   "source": [
    "In this notebook we will use Llama.cpp to execute LLMs (Large Language Models). LLama.cpp enables model loading and inference on a variety of CPU and GPU platforms including Ryzen AI through ROCm and Vulkan.\n",
    "\n",
    "# LLMs with Llama.cpp\n",
    "\n",
    "Llama.cpp supports a server/client architecture. To launch a server, run the command below. The output will show that Llama.cpp has detected an AMD GPU, connected to huggingface, retrieved an LLM (gpt-oss-20b), and begun a server. llama.cpp can serve up models from a variety of model zoos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3daa9-bf14-437b-92ec-f39bbf6b0d53",
   "metadata": {},
   "source": [
    "`llama-server -hf ggml-org/gpt-oss-20b-GGUF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8607642-7316-4cb4-a1d7-64ee9ddb8cc9",
   "metadata": {},
   "source": [
    "## Interact with a model\n",
    "\n",
    "Open a browser to https://127.0.0.1:8080 to begin a chat.\n",
    "\n",
    "\n",
    "## Benchmarking models and runtime selection\n",
    "\n",
    "On this platform, llama.cpp is compiled with ROCm and Vulkan backends. llama-bench is a utility that allows you to benchmark models under multiple backends. Run the code below in a terminal to compare the model execution under ROCm and Vulkan.\n",
    "\n",
    "`llama-bench -m ~/.cache/llama.cpp/ggml-org_gpt-oss-20b-GGUF_gpt-oss-20b-mxfp4.gguf -dev ROCm0,Vulkan0`\n",
    "\n",
    "You should see output similar to:\n",
    "\n",
    "```\n",
    "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
    "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
    "ggml_cuda_init: found 1 ROCm devices:\n",
    "  Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n",
    "ggml_vulkan: Found 1 Vulkan devices:\n",
    "ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n",
    "| model                          |       size |     params | backend    | ngl | n_batch | dev          |            test |                  t/s |\n",
    "| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | ------------ | --------------: | -------------------: |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |     256 | ROCm0        |           pp512 |      1158.96 ± 12.22 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |     256 | ROCm0        |           tg128 |         65.90 ± 0.13 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |     512 | ROCm0        |           pp512 |       1199.88 ± 5.59 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |     512 | ROCm0        |           tg128 |         65.78 ± 0.06 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |    1024 | ROCm0        |           pp512 |       1196.87 ± 9.64 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |    1024 | ROCm0        |           tg128 |         65.76 ± 0.08 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |     256 | Vulkan0      |           pp512 |       846.81 ± 20.23 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |     256 | Vulkan0      |           tg128 |         66.77 ± 0.15 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |     512 | Vulkan0      |           pp512 |        913.66 ± 5.78 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |     512 | Vulkan0      |           tg128 |         66.74 ± 0.19 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |    1024 | Vulkan0      |           pp512 |        909.28 ± 8.10 |\n",
    "| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | ROCm,Vulkan |  99 |    1024 | Vulkan0      |           tg128 |         66.82 ± 0.11 |\n",
    "\n",
    "build: 3d4e86bb (6789)\n",
    "```\n",
    "\n",
    "## VLMs\n",
    "\n",
    "You can also load other models like with additional features, such as VLMs (Vision Language Models). VLMs can run inferences on images, which are useful in robotics applications to detect objects in the robot's environment. To load Gemma3, run the following: \n",
    "\n",
    "`\n",
    "llama-server -hf ggml-org/gemma-3-4b-it-GGUF:Q4_K_M\n",
    "`\n",
    "\n",
    "Try uploading an image to the chat and asking the model about it.\n",
    "\n",
    "\n",
    "# Ollama\n",
    "\n",
    "In this section, we will look at ollama, an alternative tool for running LLMs. On Ryzen AI platforms, `ollama` uses ROCm as a backend. To run `ollama` with a model such as llama3.1, run the command below in a new terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5256b-956b-4022-88d4-cb79bc78217f",
   "metadata": {},
   "source": [
    "`ollama run llama3.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99d94e-82a9-4d5f-8de9-588b4e343f31",
   "metadata": {},
   "source": [
    "To evaluate performance of the model in realtime, run `/set verbose`. After doing so, the model will give a performance report after each prompt, like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbce79-2e7d-4c51-a29a-686ad19d059b",
   "metadata": {},
   "source": [
    "```\n",
    "total duration:       23.766780694s\n",
    "load duration:        84.270413ms\n",
    "prompt eval count:    14 token(s)\n",
    "prompt eval duration: 42.263428ms\n",
    "prompt eval rate:     331.26 tokens/s\n",
    "eval count:           751 token(s)\n",
    "eval duration:        22.796202895s\n",
    "eval rate:            32.94 tokens/s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2313db-cc41-4589-b2d9-4974abe55ca7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## References\n",
    "\n",
    "* [Llama.cpp](https://github.com/ggml-org/llama.cpp)\n",
    "* [ollama](https://ollama.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285bb33-051c-4edb-bb43-14e4efa00fde",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "Copyright© 2025 AMD, Inc SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
