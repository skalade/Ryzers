{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5dcdaa-b22d-46d8-bed4-d89148a5cdc3",
   "metadata": {},
   "source": [
    "# Running Large Language Models\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Learn to use `llama.cpp` to run inference on the AMD GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d5231-3e32-491a-851c-1c1832ee8af0",
   "metadata": {},
   "source": [
    "\n",
    "# LLMs with llama.cpp\n",
    "\n",
    "In this notebook we will use llama.cpp to execute LLMs (Large Language Models). `llama.cpp` enables model loading and inference on a variety of CPU and GPU platforms including Ryzen AI through ROCm and Vulkan.\n",
    "\n",
    "To launch `llama.cpp`, open a separate terminal and run the command below. `llama.cpp` will pull the model, detect the AMD GPU and start a chat dialog. Try asking the model a question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4ffcf-7f10-4a5e-98d3-077b09808500",
   "metadata": {},
   "source": [
    "![](images/new_terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955cbdbe-b803-442c-b621-9d4bc868764c",
   "metadata": {},
   "source": [
    "```bash\n",
    "unset HSA_OVERRIDE_GFX_VERSION\n",
    "export PATH=/ryzers/llamacpp/build/bin/:$PATH\n",
    "llama-cli -hf unsloth/Qwen3-1.7B-GGUF:Q4_K_M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4aafe-e673-44a6-82b6-82ac23e85763",
   "metadata": {},
   "source": [
    "`llama.cpp` also supports a server/client architecture amd can serve up models from a variety of model zoos. We will cover this in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8607642-7316-4cb4-a1d7-64ee9ddb8cc9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Benchmarking models and runtime selection\n",
    "\n",
    "On this platform, `llama.cpp` is compiled with ROCm and Vulkan backends. `llama-bench` is a utility that allows you to benchmark models under multiple backends. Run the code below in a terminal to compare the model execution under ROCm and Vulkan.\n",
    "\n",
    "```bash\n",
    "llama-bench -m /ryzers/.cache/llamacpp/unsloth_Llama-3.2-3B-Instruct-GGUF_Llama-3.2-3B-Instruct-Q4_K_M.gguf -dev ROCm0,Vulkan0\n",
    "```\n",
    "\n",
    "You should see output similar to:\n",
    "\n",
    "```bash\n",
    "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
    "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
    "ggml_cuda_init: found 1 ROCm devices:\n",
    "  Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n",
    "ggml_vulkan: Found 1 Vulkan devices:\n",
    "ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n",
    "| model                          |       size |     params | backend    | ngl | dev          |            test |                  t/s |\n",
    "| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------ | --------------: | -------------------: |\n",
    "| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | ROCm,Vulkan |  99 | ROCm0        |           pp512 |      4413.60 ± 38.18 |\n",
    "| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | ROCm,Vulkan |  99 | ROCm0        |           tg128 |        135.61 ± 0.78 |\n",
    "| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | ROCm,Vulkan |  99 | Vulkan0      |           pp512 |      4251.58 ± 45.03 |\n",
    "| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | ROCm,Vulkan |  99 | Vulkan0      |           tg128 |        138.42 ± 0.29 |\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300b4f1-1cc2-4049-b127-23aa7cb76155",
   "metadata": {},
   "source": [
    "The above output displays for each device, ROCm or Vulkan, the performance in tokens/second **(t/s)**. The performance tests are **pp512** and **tg128**. **pp512** is a prompt processing test and indicates how quickly the model can process prompts of 512 tokens. **tg128** is a token generation test which indicates how quickly the model can generate token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2313db-cc41-4589-b2d9-4974abe55ca7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## References\n",
    "\n",
    "* [Llama.cpp](https://github.com/ggml-org/llama.cpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285bb33-051c-4edb-bb43-14e4efa00fde",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "Copyright© 2025 AMD, Inc SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
