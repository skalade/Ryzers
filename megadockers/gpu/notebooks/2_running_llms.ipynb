{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5dcdaa-b22d-46d8-bed4-d89148a5cdc3",
   "metadata": {},
   "source": [
    "# Running Large Language Models\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Learn to use Llama.cpp to run inference on the AMD GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4aafe-e673-44a6-82b6-82ac23e85763",
   "metadata": {},
   "source": [
    "In this notebook we will use Llama.cpp to execute LLMs. LLama.cpp enables model loading and inference on a variety of CPU and GPU platforms including Ryzen AI through HIP or Vulkan.\n",
    "\n",
    "# LLMs with Llama.cpp\n",
    "\n",
    "Llama.cpp supports server/client mode. To launch a server, choose a model and run the command below. The output will show that Llama.cpp has detected an AMD GPU, connected to huggingface, retrieved the Llama3-8B model, and begun a server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0ba1827-9211-4f60-bce9-2cf2b8274ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: llama-server: command not found\n"
     ]
    }
   ],
   "source": [
    "!llama-server -hf LiquidAI/LFM2-VL-1.6B-GGUF:F16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8607642-7316-4cb4-a1d7-64ee9ddb8cc9",
   "metadata": {},
   "source": [
    "## Interact with a model\n",
    "\n",
    "Open a browser to https://127.0.0.1:8080 to begin a chat.\n",
    "\n",
    "# Ollama\n",
    "\n",
    "In this section, we will lookm at ollama, an alternative tool for running LLMs. On Ryzen AI platforms, `ollama` uses ROCm as a backend. To run `ollama` with a model such as llama3.1, run the command below in a new terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d78ca8-5af0-4d43-b349-6c6cedd651a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      "Use Ctrl + d or /bye to exit.\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ollama run llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99d94e-82a9-4d5f-8de9-588b4e343f31",
   "metadata": {},
   "source": [
    "To evaluate performance of the model in realtime, run `/set verbose`. After doing so, the model will give a performance report after each prompt, like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbce79-2e7d-4c51-a29a-686ad19d059b",
   "metadata": {},
   "source": [
    "```\n",
    "total duration:       23.766780694s\n",
    "load duration:        84.270413ms\n",
    "prompt eval count:    14 token(s)\n",
    "prompt eval duration: 42.263428ms\n",
    "prompt eval rate:     331.26 tokens/s\n",
    "eval count:           751 token(s)\n",
    "eval duration:        22.796202895s\n",
    "eval rate:            32.94 tokens/s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a616bd8-2966-4b31-8108-29356220996c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e956e633-e2dc-47ce-8e03-b4377cb930e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: lmstudio: command not found\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2313db-cc41-4589-b2d9-4974abe55ca7",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Llama.cpp](https://github.com/ggml-org/llama.cpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285bb33-051c-4edb-bb43-14e4efa00fde",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "CopyrightÂ© 2025 AMD, Inc SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
