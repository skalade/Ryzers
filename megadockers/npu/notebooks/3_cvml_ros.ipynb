{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2313db-cc41-4589-b2d9-4974abe55ca7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Ryzen AI CVML Integration with ROS 2\n",
    "\n",
    "![](images/ros_cvml.png)\n",
    "\n",
    "The Robot Operating System (ROS) is the de facto standard framework for building robotic applications. In this notebook, we'll integrate the Ryzen AI CVML library with ROS 2, demonstrating how to build NPU-accelerated vision nodes that can be used in real robotic systems.\n",
    "\n",
    "By combining CVML's optimized computer vision features with ROS 2's robust communication infrastructure, we can create power-efficient perception pipelines for autonomous robots, drones, and edge AI applications.\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Understand how to integrate NPU acceleration into ROS 2 nodes\n",
    "* Build a complete vision pipeline with multiple nodes\n",
    "* Visualize real-time depth estimation outputs\n",
    "\n",
    "## References\n",
    "\n",
    "* [Writing a Simple Publisher and Subscriber (C++)](https://docs.ros.org/en/kilted/Tutorials/Beginner-Client-Libraries/Writing-A-Simple-Cpp-Publisher-And-Subscriber.html)\n",
    "* [Using ROS 2 Launch Files](https://docs.ros.org/en/kilted/Tutorials/Intermediate/Launch/Launch-Main.html)\n",
    "* [cv_bridge](https://github.com/ros-perception/vision_opencv/tree/ros2/cv_bridge)\n",
    "\n",
    "## The `cvml_ros` Package\n",
    "\n",
    "The `cvml_ros` package provides a bridge between the CVML C++ API and ROS 2, wrapping the computer vision features into standard ROS nodes. This allows you to leverage NPU acceleration within the familiar ROS ecosystem.\n",
    "\n",
    "### Package Features\n",
    "\n",
    "The package includes three main nodes, each implementing a different CVML feature:\n",
    "\n",
    "- **depth_estimation_node**: Subscribes to image topics and publishes dense depth maps using NPU-accelerated Depth Anything V2\n",
    "- **face_detection_node**: Detects human faces in real-time, publishing bounding boxes and facial landmarks\n",
    "- **face_mesh_node**: Generates detailed 3D face meshes with 468 landmarks for applications like AR/VR and expression analysis\n",
    "\n",
    "Each node follows the standard ROS 2 publisher-subscriber pattern, making them composable and easy to integrate into existing robotics stacks.\n",
    "\n",
    "## Explore the Package Structure\n",
    "\n",
    "Let's examine the `cvml_ros` package structure to understand how it's organized. For each feature we have a Python launcher we will use to launch the nodes and the features are based on CVML C++ APIs and live in the `cvml_ros/src` directory - feel free to explore the individual implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "list-package",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvml_ros/:\n",
      "CMakeLists.txt\tinclude  launch  package.xml  README.md  scripts  src\n",
      "\n",
      "cvml_ros/include:\n",
      "cvml_ros\n",
      "\n",
      "cvml_ros/include/cvml_ros:\n",
      "depth_estimation_node.hpp  face_detection_node.hpp  face_mesh_node.hpp\n",
      "\n",
      "cvml_ros/launch:\n",
      "depth_estimation.launch.py  face_detection.launch.py  face_mesh.launch.py\n",
      "\n",
      "cvml_ros/scripts:\n",
      "video_publisher.py\n",
      "\n",
      "cvml_ros/src:\n",
      "depth_estimation_node.cpp  face_detection_node.cpp  face_mesh_node.cpp\n",
      "face_detection_main.cpp    face_mesh_main.cpp\t    main.cpp\n"
     ]
    }
   ],
   "source": [
    "!ls -R cvml_ros/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-package",
   "metadata": {},
   "source": [
    "## Building the ROS Package\n",
    "\n",
    "ROS 2 uses the `colcon` build system to compile packages. Before we can run our CVML nodes, we need to ensure the package is built with all its dependencies linked correctly.\n",
    "\n",
    "The build process compiles the C++ source files, links against the CVML libraries, and sets up the necessary ROS 2 message types and interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "build-ros-pkg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building cvml_ros package...\n",
      "Starting >>> cvml_ros\n",
      "Finished <<< cvml_ros [7.14s]\n",
      "\n",
      "Summary: 1 package finished [7.24s]\n",
      "Build complete!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This cell might take a few minutes\n",
    "\n",
    "cd cvml_ros\n",
    "\n",
    "# Check if already built\n",
    "if [ -d \"build\" ]; then\n",
    "    echo \"Package appears to be already built.\"\n",
    "    echo \"Skipping build. If you need to rebuild, run: colcon build --packages-select cvml_ros\"\n",
    "else\n",
    "    echo \"Building cvml_ros package...\"\n",
    "    source /opt/ros/kilted/setup.bash\n",
    "    cd ..\n",
    "    colcon build --packages-select cvml_ros\n",
    "    echo \"Build complete!\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bad2b-446a-4683-8855-d411a33a07a4",
   "metadata": {},
   "source": [
    "## Launch Video Publisher Node\n",
    "\n",
    "To test our depth estimation pipeline, we need a source of images. The `video_publisher.py` script is a simple ROS node that reads frames from a video file and publishes them to a ROS topic, simulating a camera stream.\n",
    "\n",
    "This loopback approach is perfect for development and testing - it gives us a consistent, repeatable video stream without needing physical hardware.\n",
    "\n",
    "### Start the Video Publisher\n",
    "\n",
    "For this step, you'll want to open a new Jupyter terminal:\n",
    "\n",
    "![](images/new_terminal.png)\n",
    "\n",
    "Copy-paste the following command into the terminal:\n",
    "\n",
    "```bash\n",
    "sudo bash -c \"source install/setup.sh && \\\n",
    "              ros2 run cvml_ros video_publisher.py \\\n",
    "                --ros-args \\\n",
    "                -p video_path:=/ryzers/RyzenAI-SW/Ryzen-AI-CVML-Library/samples/video_call.mp4 \\\n",
    "                -p topic:=/camera/image_raw\"\n",
    "```\n",
    "\n",
    "**What this does:**\n",
    "- Sources the ROS workspace setup to make our package available\n",
    "- Runs the video_publisher.py script from the cvml_ros package\n",
    "- Sets parameters: input video path and output topic name\n",
    "- Publishes frames to `/camera/image_raw` topic\n",
    "\n",
    "You should see these messages if successful:\n",
    "```\n",
    "[INFO] [1760488629.706959565] [video_publisher]: Publishing video from: /ryzers/RyzenAI-SW/Ryzen-AI-CVML-Library/samples/video_call.mp4\n",
    "[INFO] [1760488629.707166260] [video_publisher]: Publishing to topic: /camera/image_raw\n",
    "```\n",
    "\n",
    "### Verify the Node is Running\n",
    "\n",
    "Let's use ROS 2 tools to check if our video publisher is active and publishing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9e238a-f922-4487-8b64-19c1b96a26f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/camera/image_raw\n",
      "/parameter_events\n",
      "/rosout\n"
     ]
    }
   ],
   "source": [
    "!source install/setup.sh && ros2 topic list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e3a2d-717d-4e5d-999a-6a912f12cdfe",
   "metadata": {},
   "source": [
    "Perfect! The `/camera/image_raw` topic is now available in our ROS system, publishing video frames. Now we can connect our CVML depth estimation node to subscribe to this stream and process it using the NPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "launch-depth",
   "metadata": {},
   "source": [
    "## Launch Depth Estimation Pipeline\n",
    "\n",
    "Now comes the exciting part - launching our NPU-accelerated depth estimation node! This node will:\n",
    "\n",
    "1. Subscribe to `/camera/image_raw` (our video stream)\n",
    "2. Convert ROS messages to OpenCV format using cv_bridge\n",
    "3. Run depth estimation on the NPU using CVML\n",
    "4. Publish the depth maps to `/depth_estimation/depth`\n",
    "\n",
    "### Start the Depth Estimation Node\n",
    "\n",
    "Open another new terminal:\n",
    "\n",
    "![](images/new_terminal.png)\n",
    "\n",
    "Execute these commands in the new terminal:\n",
    "\n",
    "```bash\n",
    "source /opt/ros/kilted/setup.bash\n",
    "source install/setup.sh\n",
    "\n",
    "sudo bash -c \"source /opt/ros/kilted/setup.bash && \\\n",
    "              source install/setup.sh && \\\n",
    "              export LD_LIBRARY_PATH=$LD_LIBRARY_PATH && \\\n",
    "              ros2 launch cvml_ros depth_estimation.launch.py\"\n",
    "```\n",
    "\n",
    "**Note:** We need sudo permissions because the XDNA driver requires elevated privileges for NPU access. The environment variables are passed through to ensure all libraries are found.\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "The node will subscribe to our video_publisher and begin processing frames. You should observe initialization messages followed by processing logs. Key lines to look out for:\n",
    "\n",
    "```\n",
    "...\n",
    "[depth_estimation_node-1] [INFO] [1760488970.246808904] [depth_estimation_node]: Ryzen AI Depth Estimation initialized\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491575724] [depth_estimation_node]: Created publisher on: /depth_estimation/depth\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491899477] [depth_estimation_node]: Created subscriber on: /camera/image_raw\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491906947] [depth_estimation_node]: Depth estimation node started successfully\n",
    "[depth_estimation_node-1] [INFO] [1760488970.525778512] [depth_estimation_node]: Received first image: 1920x1080\n",
    "...\n",
    "```\n",
    "\n",
    "These messages confirm:\n",
    "1. CVML depth estimation context initialized successfully\n",
    "2. ROS publisher and subscriber created\n",
    "3. Node is processing incoming frames\n",
    "\n",
    "### Check Active Topics and Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab65f9-24ad-466c-b66a-263ccb74363a",
   "metadata": {},
   "source": [
    "Now let's verify the topics in our ROS system. We should see both the input camera topic and the new depth estimation output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1528e662-52a3-4c19-a837-6246e72022ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/camera/image_raw\n",
      "/depth_estimation/depth\n",
      "/parameter_events\n",
      "/rosout\n"
     ]
    }
   ],
   "source": [
    "!source install/setup.sh && ros2 topic list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddafbaf-6254-4748-ab9e-696f30ffe068",
   "metadata": {},
   "source": [
    "Excellent! We now have a complete pipeline:\n",
    "- `/camera/image_raw` - Input video stream\n",
    "- `/depth_estimation/depth` - NPU-generated depth maps\n",
    "\n",
    "### Verify NPU Utilization\n",
    "\n",
    "Let's confirm the NPU is actually being used for inference. This is really important - we want to ensure our depth estimation is running on the NPU hardware, not falling back to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c008eee-40e5-4882-ab1b-98af3c8a81a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------\n",
      "[0000:c6:00.1] : NPU Strix Halo\n",
      "--------------------------------\n",
      "AIE Partitions\n",
      "  Total Memory Usage: N/A\n",
      "  Partition Index   : 0\n",
      "    Columns: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "    HW Contexts:\n",
      "      |PID                 |Ctx ID     |Submissions |Migrations  |Err  |Priority |\n",
      "      |Process Name        |Status     |Completions |Suspensions |     |GOPS     |\n",
      "      |Memory Usage        |Instr BO   |            |            |     |FPS      |\n",
      "      |                    |           |            |            |     |Latency  |\n",
      "      |====================|===========|============|============|=====|=========|\n",
      "      |11624               |1          |1314        |0           |0    |Normal   |\n",
      "      |N/A                 |Active     |1314        |6           |     |9        |\n",
      "      |N/A                 |1712 KB    |            |            |     |N/A      |\n",
      "      |                    |           |            |            |     |N/A      |\n",
      "      |--------------------|-----------|------------|------------|-----|---------|\n"
     ]
    }
   ],
   "source": [
    "!sudo /opt/xilinx/xrt/bin/xrt-smi examine --report aie-partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aca38e-128c-47c9-a987-5d5f7ce2c0f3",
   "metadata": {},
   "source": [
    "Perfect! The NPU hardware context shows active processing:\n",
    "- **Active status**: NPU is currently running inference\n",
    "- **Submissions/Completions**: Shows frame processing throughput\n",
    "- **Columns [0-7]**: Full NPU array is allocated for depth estimation\n",
    "- **Memory Usage**: Shows the model and intermediate buffers loaded on NPU\n",
    "\n",
    "We now have a complete pipeline: **MP4 → video_publisher → depth_estimation_node (NPU)** \n",
    "\n",
    "What's missing? Visualization! We need a way to see the depth maps being generated in real-time.\n",
    "\n",
    "## Visualize the Pipeline\n",
    "\n",
    "To see what our depth estimation node is producing, we'll use `web_video_server` - a ROS package that provides HTTP endpoints for streaming ROS image topics to web browsers or Jupyter notebooks.\n",
    "\n",
    "### Start the Web Video Server\n",
    "\n",
    "Open another terminal and run:\n",
    "\n",
    "```bash\n",
    "sudo bash -c \"source install/setup.sh && \\\n",
    "              ros2 run web_video_server web_video_server --ros-args -p port:=8080 -p address:=0.0.0.0\"\n",
    "```\n",
    "\n",
    "This creates a web server on port 8080 that can serve snapshots and streams from any image topic in our system.\n",
    "\n",
    "### Check Available Topics\n",
    "\n",
    "Let's confirm all our topics are still active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd97301-347d-4e29-ae2e-7794184a8881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/camera/image_raw\n",
      "/depth_estimation/depth\n",
      "/parameter_events\n",
      "/rosout\n"
     ]
    }
   ],
   "source": [
    "!source install/setup.sh && ros2 topic list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513b78a-e307-4538-8b0a-c02610090c1b",
   "metadata": {},
   "source": [
    "### Use `ipywidgets` to Visualize the Published Streams\n",
    "\n",
    "We will use `ipywidgets` and HTML display to visualize the node outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ee5410-24a1-4c2f-9ae1-e061811653bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d09df2c2ab6417abeed0b762e38eed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc95df57519b485bb62c14ee8efc3f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Refresh Images', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "from ipywidgets import Button, Output\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "out = Output()\n",
    "display(out)\n",
    "\n",
    "def update_display(b=None):\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        try:\n",
    "            camera = requests.get('http://localhost:8080/snapshot?topic=/camera/image_raw', timeout=3).content\n",
    "            depth = requests.get('http://localhost:8080/snapshot?topic=/depth_estimation/depth', timeout=3).content\n",
    "            \n",
    "            cam_b64 = base64.b64encode(camera).decode()\n",
    "            depth_b64 = base64.b64encode(depth).decode()\n",
    "            \n",
    "            display(HTML(f'''\n",
    "            <div style=\"display: flex; gap: 20px;\">\n",
    "                <div>\n",
    "                    <h3>Camera</h3>\n",
    "                    <img src=\"data:image/jpeg;base64,{cam_b64}\" width=\"480\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <h3>Depth (NPU)</h3>\n",
    "                    <img src=\"data:image/jpeg;base64,{depth_b64}\" width=\"480\">\n",
    "                </div>\n",
    "            </div>\n",
    "            '''))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "button = Button(description=\"Refresh Images\")\n",
    "button.on_click(update_display)\n",
    "display(button)\n",
    "update_display()  # Initial display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations! You have finished the NPU portion of the workshop! You are now familiar with the following:\n",
    "* You can inspect the NPU device status for debugging.\n",
    "* Are familiar with the CVML library features and how to compile and run them on the NPU.\n",
    "* You can integrate NPU application within a ROS 2 system.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "* Try launching other nodes like face detection or face mesh.\n",
    "* Check NPU status as you launch new nodes, can the device handle multiple applications running simultaneously?\n",
    "* If running locally, try other ROS 2 visualization nodes like `rviz2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright© 2025 AMD, Inc SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
