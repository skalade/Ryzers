{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2313db-cc41-4589-b2d9-4974abe55ca7",
   "metadata": {},
   "source": [
    "# Ryzen AI CVML Integration with ROS\n",
    "\n",
    "In this notebook, we'll integrate the Ryzen AI CVML library with ROS 2 to create NPU-accelerated vision nodes for robotics applications.\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Understand the `ryzen_ai_cvml` ROS package architecture\n",
    "* Launch CVML nodes with ROS 2\n",
    "* Visualize depth estimation and face detection outputs in ROS\n",
    "* Learn how to integrate NPU acceleration into your own ROS projects\n",
    "\n",
    "## References\n",
    "\n",
    "* [Writing a Simple Publisher and Subscriber (C++)](https://docs.ros.org/en/kilted/Tutorials/Beginner-Client-Libraries/Writing-A-Simple-Cpp-Publisher-And-Subscriber.html)\n",
    "* [Using ROS 2 Launch Files](https://docs.ros.org/en/kilted/Tutorials/Intermediate/Launch/Launch-Main.html)\n",
    "* [cv_bridge](https://github.com/ros-perception/vision_opencv/tree/ros2/cv_bridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "package-overview",
   "metadata": {},
   "source": [
    "## The `ryzen_ai_cvml` ROS Package\n",
    "\n",
    "The `ryzen_ai_cvml` package wraps the CVML C++ API into ROS 2 nodes. It provides:\n",
    "\n",
    "### Nodes\n",
    "- **depth_estimation_node**: Subscribes to image topics, publishes depth maps\n",
    "- **face_detection_node**: Detects faces and publishes bounding boxes + landmarks\n",
    "- **face_mesh_node**: Generates 3D face meshes (468 landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-package",
   "metadata": {},
   "source": [
    "## Explore the Package Structure\n",
    "\n",
    "Let's look at what's in the `ryzen_ai_cvml` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "list-package",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ryzen_ai_cvml/:\n",
      "CMakeLists.txt\tinclude  launch  package.xml  README.md  scripts  src\n",
      "\n",
      "ryzen_ai_cvml/include:\n",
      "ryzen_ai_cvml\n",
      "\n",
      "ryzen_ai_cvml/include/ryzen_ai_cvml:\n",
      "depth_estimation_node.hpp  face_detection_node.hpp  face_mesh_node.hpp\n",
      "\n",
      "ryzen_ai_cvml/launch:\n",
      "depth_estimation.launch.py  face_detection.launch.py  face_mesh.launch.py\n",
      "\n",
      "ryzen_ai_cvml/scripts:\n",
      "video_publisher.py\n",
      "\n",
      "ryzen_ai_cvml/src:\n",
      "depth_estimation_node.cpp  face_detection_node.cpp  face_mesh_node.cpp\n",
      "face_detection_main.cpp    face_mesh_main.cpp\t    main.cpp\n"
     ]
    }
   ],
   "source": [
    "!ls -R ryzen_ai_cvml/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-package",
   "metadata": {},
   "source": [
    "## Building the ROS Package\n",
    "\n",
    "If the package isn't built yet, we need to build it with colcon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "build-ros-pkg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ryzen_ai_cvml package...\n",
      "Starting >>> ryzen_ai_cvml\n",
      "Finished <<< ryzen_ai_cvml [0.40s]\n",
      "\n",
      "Summary: 1 package finished [0.48s]\n",
      "Build complete!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This cell might take a few minutes\n",
    "\n",
    "cd ryzen_ai_cvml\n",
    "\n",
    "# Check if already built\n",
    "if [ -d \"build\" ]; then\n",
    "    echo \"Package appears to be already built.\"\n",
    "    echo \"Skipping build. If you need to rebuild, run: colcon build --packages-select ryzen_ai_cvml\"\n",
    "else\n",
    "    echo \"Building ryzen_ai_cvml package...\"\n",
    "    source /opt/ros/kilted/setup.bash\n",
    "    cd ..\n",
    "    colcon build --packages-select ryzen_ai_cvml\n",
    "    echo \"Build complete!\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bad2b-446a-4683-8855-d411a33a07a4",
   "metadata": {},
   "source": [
    "## Launch Loopback Video Publisher\n",
    "\n",
    "For this part you will want to open a new jupyter terminal\n",
    "\n",
    "![](images/new_terminal.png)\n",
    "\n",
    "Type in the following:\n",
    "\n",
    "```bash\n",
    "source install/setup.bash\n",
    "\n",
    "ros2 run ryzen_ai_cvml video_publisher.py \\\n",
    "    --ros-args \\\n",
    "    -p video_path:=/ryzers/RyzenAI-SW/Ryzen-AI-CVML-Library/samples/video_call.mp4 \\\n",
    "    -p topic:=/camera/image_raw\n",
    "```\n",
    "\n",
    "You should see these messages if successful:\n",
    "```\n",
    "[INFO] [1760488629.706959565] [video_publisher]: Publishing video from: /ryzers/RyzenAI-SW/Ryzen-AI-CVML-Library/samples/video_call.mp4\n",
    "[INFO] [1760488629.707166260] [video_publisher]: Publishing to topic: /camera/image_raw\n",
    "```\n",
    "\n",
    "Let's see if ROS detects our new node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca9e238a-f922-4487-8b64-19c1b96a26f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/video_publisher\n"
     ]
    }
   ],
   "source": [
    "!ros2 node list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e3a2d-717d-4e5d-999a-6a912f12cdfe",
   "metadata": {},
   "source": [
    "Excellent! We have a video_publisher node active in our system. Now let's use the CVML depth estimation node to subscribe to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "launch-depth",
   "metadata": {},
   "source": [
    "## Launch Depth Estimation Pipeline\n",
    "\n",
    "For this we will want yet another new terminal\n",
    "\n",
    "![](images/new_terminal.png)\n",
    "\n",
    "Now type these commands into the new terminal:\n",
    "\n",
    "```bash\n",
    "source /opt/ros/kilted/setup.bash\n",
    "source install/setup.bash\n",
    "ros2 launch ryzen_ai_cvml depth_estimation.launch.py\n",
    "```\n",
    "\n",
    "The node will subscribe to our video_publisher and publish depth maps on `/depth/image`. Once all goes well you should observe the following:\n",
    "\n",
    "```\n",
    "[INFO] [launch]: All log files can be found below /root/.ros/log/2025-10-15-00-42-50-071596-stxh-rad-126898\n",
    "[INFO] [launch]: Default logging verbosity is set to INFO\n",
    "[INFO] [depth_estimation_node-1]: process started with pid [126902]\n",
    "[depth_estimation_node-1] [INFO] [1760488970.197557193] [depth_estimation_node]: Input topic: /camera/image_raw\n",
    "[depth_estimation_node-1] [INFO] [1760488970.197609302] [depth_estimation_node]: Output topic: /depth_estimation/depth\n",
    "[depth_estimation_node-1] [INFO] time:185810749 thread:132226191791872 AMD CVML SDK: 0.0.0-dev\n",
    "[depth_estimation_node-1] [INFO] time:185810749 thread:132226191791872 Any GPU inference will use AMD Radeon Graphics (RADV GFX1151)[0]\n",
    "[depth_estimation_node-1] [INFO] time:185810749 thread:132226191791872 Any GPU inference will use AMD Radeon Graphics (RADV GFX1151)[0]\n",
    "[depth_estimation_node-1] [INFO] time:185810758 thread:132226191791872 [Depth Estimation] Using ONNX engine, NPU backend\n",
    "[depth_estimation_node-1] [INFO] [1760488970.246808904] [depth_estimation_node]: Ryzen AI Depth Estimation initialized\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491575724] [depth_estimation_node]: Created publisher on: /depth_estimation/depth\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491899477] [depth_estimation_node]: Created subscriber on: /camera/image_raw\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491906947] [depth_estimation_node]: Depth estimation node started successfully\n",
    "[depth_estimation_node-1] [INFO] [1760488970.525778512] [depth_estimation_node]: Received first image: 1920x1080\n",
    "[depth_estimation_node-1] WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
    "[depth_estimation_node-1] I20251015 00:42:50.540738 126927 vitisai_compile_model.cpp:1143] Vitis AI EP Load ONNX Model Success\n",
    "[depth_estimation_node-1] I20251015 00:42:50.540784 126927 vitisai_compile_model.cpp:1144] Graph Input Node Name/Shape (1)\n",
    "[depth_estimation_node-1] I20251015 00:42:50.540791 126927 vitisai_compile_model.cpp:1148]       efficient_Unet::input_0_nhwc : [1x256x256x3]\n",
    "[depth_estimation_node-1] I20251015 00:42:50.540795 126927 vitisai_compile_model.cpp:1154] Graph Output Node Name/Shape (1)\n",
    "[depth_estimation_node-1] I20251015 00:42:50.540797 126927 vitisai_compile_model.cpp:1158]       2196_nhwc : [1x256x256x1]\n",
    "[depth_estimation_node-1] [Vitis AI EP] No. of Operators :   CPU     2    NPU   616 \n",
    "[depth_estimation_node-1] [Vitis AI EP] No. of Subgraphs :   NPU     1 Actually running on NPU     1 \n",
    "[depth_estimation_node-1] [INFO] time:185811364 thread:132224952436416 [ONNX VAI] Session created\n",
    "[depth_estimation_node-1] [INFO] [1760488970.872380363] [depth_estimation_node]: Published first depth map\n",
    "```\n",
    "\n",
    "Key lines here are\n",
    "```\n",
    "[depth_estimation_node-1] [INFO] [1760488970.246808904] [depth_estimation_node]: Ryzen AI Depth Estimation initialized\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491575724] [depth_estimation_node]: Created publisher on: /depth_estimation/depth\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491899477] [depth_estimation_node]: Created subscriber on: /camera/image_raw\n",
    "[depth_estimation_node-1] [INFO] [1760488970.491906947] [depth_estimation_node]: Depth estimation node started successfully\n",
    "[depth_estimation_node-1] [INFO] [1760488970.525778512] [depth_estimation_node]: Received first image: 1920x1080\n",
    "```\n",
    "\n",
    "### Check Active Topics and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9d0a688-511b-4278-a5e8-0ebccd7717fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/depth_estimation_node\n",
      "/video_publisher\n"
     ]
    }
   ],
   "source": [
    "!ros2 node list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab65f9-24ad-466c-b66a-263ccb74363a",
   "metadata": {},
   "source": [
    "Now for the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1528e662-52a3-4c19-a837-6246e72022ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/camera/image_raw\n",
      "/depth_estimation/depth\n",
      "/parameter_events\n",
      "/rosout\n"
     ]
    }
   ],
   "source": [
    "!ros2 topic list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddafbaf-6254-4748-ab9e-696f30ffe068",
   "metadata": {},
   "source": [
    "Also, let's make sure the NPU is actually being utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c008eee-40e5-4882-ab1b-98af3c8a81a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------\n",
      "[0000:c6:00.1] : NPU Strix Halo\n",
      "--------------------------------\n",
      "AIE Partitions\n",
      "  Total Memory Usage: N/A\n",
      "  Partition Index   : 0\n",
      "    Columns: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "    HW Contexts:\n",
      "      |PID                 |Ctx ID     |Submissions |Migrations  |Err  |Priority |\n",
      "      |Process Name        |Status     |Completions |Suspensions |     |GOPS     |\n",
      "      |Memory Usage        |Instr BO   |            |            |     |FPS      |\n",
      "      |                    |           |            |            |     |Latency  |\n",
      "      |====================|===========|============|============|=====|=========|\n",
      "      |189779              |1          |8257        |0           |0    |Normal   |\n",
      "      |N/A                 |Active     |8257        |37          |     |9        |\n",
      "      |N/A                 |1712 KB    |            |            |     |N/A      |\n",
      "      |                    |           |            |            |     |N/A      |\n",
      "      |--------------------|-----------|------------|------------|-----|---------|\n"
     ]
    }
   ],
   "source": [
    "!/opt/xilinx/xrt/bin/xrt-smi examine --report aie-partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aca38e-128c-47c9-a987-5d5f7ce2c0f3",
   "metadata": {},
   "source": [
    "Great, now we have a mp4 -> video_publisher -> depth_estimation pipeline! What are we missing? We need to visualize the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6ba99-2b1f-4e01-b947-e07212f5d0ae",
   "metadata": {},
   "source": [
    "## Visualize the Pipeline\n",
    "\n",
    "```bash\n",
    "ros2 run web_video_server web_video_server --ros-args -p port:=8080 -p address:=0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbd97301-347d-4e29-ae2e-7794184a8881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/camera/image_raw\n",
      "/depth_estimation/depth\n",
      "/parameter_events\n",
      "/rosout\n"
     ]
    }
   ],
   "source": [
    "!ros2 topic list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d72b80-64db-488f-ac58-d0aeaece1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(f'''\n",
    "    <img src=\"http://localhost:8080/stream?topic=/depth_estimation/depth&type=mjpeg\" \n",
    "         width=\"640\" height=\"480\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7fc87-687b-439d-ae2b-1922cc8c1bce",
   "metadata": {},
   "source": [
    "We can also check out the raw video stream by replacing the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242db01c-5170-47fd-8518-b31282cb5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f'''\n",
    "    <img src=\"http://localhost:8080/stream?topic=/camera/image_raw&type=mjpeg\" \n",
    "         width=\"640\" height=\"480\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Integration Pattern\n",
    "\n",
    "1. **CVML Context** → Create and configure backend\n",
    "2. **CVML Feature** → Initialize the vision feature (depth, face, etc.)\n",
    "3. **ROS Subscriber** → Receive image messages\n",
    "4. **cv_bridge** → Convert between ROS and OpenCV formats\n",
    "5. **CVML Processing** → NPU-accelerated inference\n",
    "6. **ROS Publisher** → Publish results\n",
    "\n",
    "### Benefits of NPU in ROS\n",
    "\n",
    "- **Power Efficient**: Lower power draw compared to GPU/CPU\n",
    "- **Dedicated Hardware**: Frees up CPU/GPU for other tasks\n",
    "- **Real-time Performance**: ~30 FPS for depth estimation\n",
    "- **Easy Integration**: Same ROS patterns as any other vision node\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Backend Selection**: Use parameters to allow runtime backend switching\n",
    "2. **Error Handling**: Check NPU availability, handle fallback to CPU\n",
    "3. **Performance Monitoring**: Use `xrt-smi` to verify NPU utilization\n",
    "4. **Resource Management**: Properly release CVML contexts on shutdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You've now learned how to:\n",
    "- ✅ Validate and use the NPU with `xrt-smi`\n",
    "- ✅ Run CVML samples on different backends\n",
    "- ✅ Integrate CVML with ROS 2 for robotics applications\n",
    "\n",
    "### Further Exploration\n",
    "\n",
    "- Try different CVML features (face mesh, pose estimation)\n",
    "- Create custom ROS nodes with CVML integration\n",
    "- Combine multiple CVML features in a single pipeline\n",
    "- Benchmark NPU vs GPU vs CPU for your specific workloads\n",
    "- Build custom NPU applications with MLIR-AIE (advanced)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Ryzen AI Documentation](https://ryzenai.docs.amd.com/)\n",
    "- [XDNA Driver GitHub](https://github.com/amd/xdna-driver)\n",
    "- [RyzenAI-SW GitHub](https://github.com/amd/RyzenAI-SW)\n",
    "- [ROS 2 Documentation](https://docs.ros.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright© 2025 AMD, Inc SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
